---
jupyter:
  jupytext:
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.14.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- LTeX: language=fr -->

<!-- #region slideshow={"slide_type": "slide"} -->
Cours 5‚ÄØ: Parser des documents balis√©s avec `lxml` et BeautifulSoup
==================================================================

**Lo√Øc Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

<!-- #endregion -->

```python
from IPython.display import display
```

## Documents balis√©s

## `lxml`‚ÄØ? Beautiful Soup‚ÄØ?

`lxml`. Beautiful Soup.

[`lxml`](http://lxml.de/) est un parseur de documents balis√©s, fonctionnant via un *binding* de la
biblioth√®que [`libxml2`](http://xmlsoft.org/). Il est con√ßu pour √™tre rapide (*tr√®s* rapide) et tr√®s
pr√®s du standard. [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/) est une
*interface* d'acc√®s pour HTML (et XML) con√ßu pour √™tre la plus souple et facile d'utilisation
possible (un peu la m√™me philosophie que `requests`) et pour rendre facile le travail sur des
documents potentiellement mal form√©s.

Installons ces modules, soit dans votre terminal avec `pip`, soit en ex√©cutant la cellule de code
suivante. Comme d'habitude, il est vivement recommand√© de travailler pour ce cours dans un
environnement virtuel et si vous avez install√© le [requirements.txt](../../requirements.txt) de ce
cours, ces modules sont d√©j√† install√©s. Nous aurons √©galement besoin de `requests` [que nous avons
d√©j√† utilis√©](../04-requests/requests-slides.py.md) et plus anecdotiquement de `matplotlib`.


```python
%pip install -U beautifulsoup4 lxml matplotlib requests
```

```python
from bs4 import BeautifulSoup
import lxml
import requests
```

## Parser du HTML

[On ne parse pas du HTML avec de regex](https://stackoverflow.com/a/1732454)

(sauf quand on le fait quand m√™me)

(mais pas ici)

(non mais)

Beautiful Soup permet de parser simplement du contenu HTML. M√™me si le contenu est mal form√©, le
module reconstitue un arbre et offre des fonctions faciles √† utiliser pour le parcourir ou y
rechercher des √©l√©ments.

Beautiful Soup n'est pas un parseur, mais *utilise* des parseurs.

Nous travaillerons directement avec du contenu en ligne. Fini les exercices bidons, cette fois nous
allons nous confronter √† une question essentielle : combien d'accordages *open tuning* Neil Young
utilise et comment sont-ils r√©partis dans son ≈ìuvre ?  
On trouve les infos sur les chansons de Neil Young et les accordages sur le fabuleux site
[songx.se](http://songx.se/index.php) (le site ayant chang√© d'interface, nous utiliserons une
archive de [Wayback Machine](http://web.archive.org/))


Avec `requests` pour r√©cup√©rer les donn√©es, nous allons pouvoir instancier un objet Beautiful Soup
sans trop d'efforts

```python
import requests
from bs4 import BeautifulSoup

url = "http://web.archive.org/web/20180430090903/http://songx.se/index.php" # le lien vers le site
html = requests.get(url) # on r√©cup√®re le contenu
soup = BeautifulSoup(html.text, 'lxml') # on cr√©e un objet pour traiter la page
```

Voil√† nous avons maintenant un objet `soup` de classe [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#beautifulsoup).

La [doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) est tr√®s claire.

Chercher un √©l√©ment avec le tag `title`

```python
e = soup.title
e
```

R√©cup√©rer son tag

```python
e.name
```

R√©cup√©rer son contenu textuel

```python
e.string
```

<!-- #region -->
Les informations qui nous int√©ressent sont contenues dans des √©l√©ments comme celui-ci (format√© pour
une meilleure lisibilit√©) :

```html
<div class="songrow">
    <a href="?song=505">Clementine</a>
    <small>(cover)</small>
    <div style="float:right;">EADGBE</div>
</div>
```

O√π on trouve le nom de la chanson (`Clementine`) et l'accord utilis√© (`EADGBE`)
<!-- #endregion -->

### üé∂ Exo 1 üé∂

1\. Affichez les titres et les tunings des 10 premi√®res chansons en utilisant la m√©thode
[`find_all`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all). La m√©thode renvoie un
it√©rable.

```python
for item in soup.find_all([...]):
    print(item.[...], item.[...])
```

2\. Cr√©ez un `dict` appel√© `tunings` qui classe les chansons par tuning (autrement dit qui associe √†
un tuning la liste des morceaux qui l'utilisent). (On peut utiliser plus sympa qu'un b√™te `dict`).
Lisez bien [la doc de `find_all`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all).

```python
from typing import Dict, List
tunings: Dict[str, List[str]] = dict()
for item in soup.find_all([...]):
    song_title = item.[...]
    tuning = item.[...]
    [...]
```

(`typing` c'est quoi‚ÄØ? Qu'est-ce que √ßa veut dire `Dict[str, List[str]]`‚ÄØ? On en reparlera si on a le
temps mais vous pouvez prendre de l'avance [sur Real
Python](https://realpython.com/python-type-checking/))

3\. 'Harvest Moon' utilise l'accordage DADGBE, y en a-t'il d'autres ?

```python
print([...])
```

4\. Affichez les accordages et le nombre de chansons pour chaque accordage, le tout tri√© par nombre de
chansons d√©croissant. Pour cela, cous utiliserez la m√©thode `sorted` ainsi que l'argument mot-cl√©
`key` (des exemples [ici](https://wiki.python.org/moin/HowTo/Sorting#Key_Functions)) :

```python
for tuning in sorted(tunings.keys(), key=[...]):
    print([...])
```

Allez hop un histogramme

```python
%matplotlib inline
import matplotlib.pyplot as plt
values = [len(tunings[x]) for x in tunings]
values
plt.bar(range(0, len(values)), values)
plt.xticks(range(0, len(values)), tunings.keys(), rotation=17)

plt.show()
```

5\. Une chanson a un tuning un peu particulier compar√© aux autres (indice, c'est une question de
taille). Trouvez le tuning qui est diff√©rent des autres et donnez la chanson qui lui correspond.
_Attention_, "b" signifie "b√©mol", il ne s'agit pas d'une note pour l'accordage (contrairement √† A,
B, C, D, E, F et G) !


## Parser du XML

Nous allons travailler sur un fichier au format [TEI](http://www.tei-c.org/) extrait du corpus
[*Corpus 14*](https://hdl.handle.net/11403/corpus14/v1).

Le fichier se nomme [`josephine-1-150119.xml`](data/josephine-1-150119.xml). Il s'agit d'une
lettre d'une femme de soldat √† son √©poux. Les chemins du notebook devraient fonctionner sur Binder,
pour bosser en local, vous pouvez le r√©cup√©rer sur
[GitHub](https://raw.githubusercontent.com/LoicGrobol/web-interfaces/main/data/josephine-1-150119.xml)

Nous allons extraire du fichier TEI les informations suivantes‚ÄØ:

- titre (`/TEI/teiHeader/fileDesc/titleStmt/title`)
- source (`/TEI/teiHeader/fileDesc/sourceDesc/p`)
- contenu de la lettre (`/TEI/text/body`)

Vous pouvez trouver des indications sur les √©l√©ments de la TEI
[ici](http://www.tei-c.org/release/doc/tei-p5-doc/fr/html/) (√ßa pourra √™tre utile pour les
questions)

### Avec lxml

Pourquoi `lxml` et pas `xml.etree.ElementTree` ? Parce que : [1](http://lxml.de/intro.html) et
surtout [2](http://lxml.de/performance.html).

La bonne nouvelle, c'est que votre code sera aussi compatible avec `xml.etree.ElementTree` ou
`xml.etree.cElementTree` parce que xml utilise l'API ElementTree. Sauf pour la m√©thode `xpath` qui
est propre √† `libxml`.

```python
from lxml import etree
tree = etree.parse('data/josephine-1-150119.xml')
root = tree.getroot()

# Parcours des enfants de la racine (commentaires et √©l√©ments)
for child in root:
    print(child.tag)
```

### Avec des requ√™tes `ElementPath`

Le fichier utilise l'[espace de nom](https://fr.wikipedia.org/wiki/Espace_de_noms_XML) TEI : `<TEI
xmlns="http://www.tei-c.org/ns/1.0">`, nous devrons l'indiquer dans nos instructions de recherche.

Nous pouvons r√©cup√©rer un √©l√©ment particulier qui correspond √† un chemin : par exemple, pour
r√©cup√©rer le *header* TEI dont le chemin est `/TEI/teiHeader`

la m√©thode `find` renvoie le premier √©l√©ment qui correspond au chemin argument (`ElementPath` et non
`xpath`)

```python
header = root.find("./tei:teiHeader", namespaces={'tei':"http://www.tei-c.org/ns/1.0"})
header
```

### üß≠ Exo üß≠

1\. R√©cup√©rez le titre et affichez son tag XML ainsi que son contenu textuel (`/TEI/teiHeader/fileDesc/titleStmt/title`)

```python
title = root.find("[...]", namespaces={'tei':"http://www.tei-c.org/ns/1.0"})
print("Tag : {}".format(title.tag))
print("Texte : {}".format(title.text))
```

2\. Idem pour la source (√©l√©ment `sourceDesc`) :

```python
source = root.find("[...]", namespaces={'tei':"http://www.tei-c.org/ns/1.0"})
print("Tag : {}".format(source.tag))
print("Texte : {}".format(source.text))
```

### Avec des requ√™tes xpath

`lxml` a aussi une m√©thode [xpath](https://lxml.de/xpathxslt.html) qui permet d'utiliser directement
des [expressions xpath](https://www.w3schools.com/xml/xpath_syntax.asp) (sans oublier les espaces de
noms pour notre fichier)‚ÄØ:

```python
source = root.xpath(
    "/tei:TEI/tei:teiHeader/tei:fileDesc/tei:sourceDesc/tei:p",
    namespaces={'tei':'http://www.tei-c.org/ns/1.0'},
)
print(f"xpath retourne un objet de type {type(source)}")
print(source[0].text)
```

Ou comme ceci

```python
source = root.xpath(
    "/tei:TEI/tei:teiHeader/tei:fileDesc/tei:sourceDesc/tei:p/text()",
    namespaces={'tei':'http://www.tei-c.org/ns/1.0'},
)
print(source[0])
```

Pour le contenu il faut ruser. La difficult√© ici tient √† l'utilisation d'√©lements `<lb/>` de type
[milestones](http://www.tei-c.org/release/doc/tei-p5-doc/fr/html/CO.html#CORS5) pour noter les
retours √† la ligne :

```xml
<p>
je reponse a ton aimableux lettres<lb/>
que nous a fait plaisir en naprenas<lb/>
que tu et enbonne santes car il<lb/>
anais de maime pour nous<lb/>
</p>
```

### ü•≤ Exo ü•≤

1\. R√©cup√©rez dans un premier temps l'ensemble des balises `<p>` en utilisant la m√©thode
[findall](http://effbot.org/zone/element.htm#searching-for-subelements). la m√©thode `findall`
renvoie une liste avec tous les √©l√©ments correspondant au chemin argument.

```python
body = root.findall("./[...]", namespaces={'tei':"http://www.tei-c.org/ns/1.0"})
for elem in body: # tout le texte ne s'affichera pas, c'est normal !
    print(elem.text)
```

Ici on ne r√©cup√®re que les n≈ìuds `text` pr√©c√©dant les √©l√©ments `<lb/>`.

2\. Utilisez la fonction `xpath` pour r√©cup√©rer tous les n≈ìuds text du corps de la lettre. Vous
int√©grerez dans votre requ√™te la fonction `text` (vue un peu plus haut) dans votre chemin xpath
(vous pouvez _aussi_ fouiller [par ici](https://lxml.de/xpathxslt.html) pour avoir de la
documentation suppl√©mentaire).

```python
body = root.xpath("[...]", namespaces={'tei':"http://www.tei-c.org/ns/1.0"})
for text in body:
    print(text, end="")
```

3\. √âcrivez une requ√™te xpath pour r√©cup√©rer tous les √©l√©ments ratur√©s de la lettre de Jos√©phine.

## Avec DOM

L'API `ElementTree` est propre √† Python, `DOM` ([le site officiel](https://www.w3.org/DOM/) et [des
informations en fran√ßais](https://developer.mozilla.org/fr/docs/Web/API/Document_Object_Model)) est
une API ind√©pendante d'un langage de programmation. Il existe des impl√©mentations `DOM` dans la
plupart des langages de programmation modernes.  

```python
from xml.dom import minidom
dom = minidom.parse("data/josephine-1-150119.xml")
dom
```

```python
# un seul √©l√©ment 'title' dans le document
title = dom.getElementsByTagNameNS("http://www.tei-c.org/ns/1.0", 'title')[0]
title
```

`title` est un objet `Element`, pour acc√©der au contenu textuel il faut r√©cup√©rer le n≈ìud texte

```python
print(title) 
print(title.lastChild.nodeName)
print(title.lastChild.nodeValue)
```

Idem pour la source, sauf qu'on ne peut pas se permettre de rechercher tous les √©l√©ments `p`.  
Il faut trouver l'√©l√©ment `p` fils de `sourceDesc`

```python
sourceDesc = dom.getElementsByTagNameNS("http://www.tei-c.org/ns/1.0", 'sourceDesc')[0]
for node in sourceDesc.childNodes:
    if node.localName == "p":
        print(node.lastChild.nodeValue)
```

Et maintenant le contenu et ses √©l√©ments milestones

### üòå Exo üòå

Pour garder la forme, vous r√©√©crirez les boucles `for` suivies de `if` en listes en intension.

```python
body = dom.getElementsByTagNameNS("http://www.tei-c.org/ns/1.0", 'body')[0]
for node in body.childNodes:
    if node.localName in ("p", "opener"):
        for in_node in node.childNodes:
            if in_node.nodeName == "#text":
                print(in_node.nodeValue, end="")
```

## Avec lxml et Beautiful Soup

```python
from bs4 import BeautifulSoup

with open("data/josephine-1-150119.xml") as fp:
    soup = BeautifulSoup(fp, 'lxml')
```

```python
soup.title.text
```

```python
soup.sourcedesc.p.text
```

Pour le contenu de la lettre il y a la merveilleuse fonction `get_text()`

```python
soup.get_text?
```

```python
text = soup.find("text")
print(text.getText())
```

`lxml` est rapide, Beautiful Soup simple √† utiliser. Le combo diablement efficace.

Il y a un autre module super pour le web que nous ne verrons pas dans cette s√©ance mais que je me
dois de vous indiquer :¬†[Selenium](https://selenium-python.readthedocs.io/) Selenium va vous
permettre d'automatiser des actions sur un navigateur. Je vous conseille d'essayer, c'est assez
plaisant de voir votre navigateur pilot√© par un script. C'est aussi g√©nial pour tester
automatiquement les interfaces web que vous d√©veloppez

## ü§î Exo d'application ü§î

> Wikipedia trivia: if you take any article, click on the first link in the article text not in
> parentheses or italics, **and then repeat**, you will eventually end up at "Philosophy". ([xkcd
> #903](https://xkcd.com/903/))

- V√©rifiez sur une page ou deux si c'est vrai
- √âcrivez un script qui prend en argument de ligne de commande un nom de page Wikip√©dia (en anglais,
  sauf si vous aimez l'aventure) et donne le nombre de sauts n√©cessaire pour arriver √† la page
  *Philosophy* ou une erreur si la page en question n'existe pas
- Si vous √™tes tr√®s d√©termin√©‚ãÖe‚ãÖs, faites un script qui prend en entr√©e des pages de Wikip√©dia et
  produit le graphe (orient√©) des pages obtenues en suivant √† chaque fois le premier lien de chaque
  page, et ce jusqu'√† retomber sur une page d√©j√† visit√©e. On pourra par exemple utiliser
  [NetworkX](https://networkx.org/documentation/latest/reference/drawing.html), un visualiseur
  interactif comme [pyvis](https://pyvis.readthedocs.io/en/latest/tutorial.html), [un wrapper de
  graphviz](https://graphviz.readthedocs.io) ou encore g√©n√©rer directement des fichiers dot.
