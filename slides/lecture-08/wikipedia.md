---
jupyter:
  jupytext:
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.13.0
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

[comment]: <> "LTeX: language=fr"

<!-- #region slideshow={"slide_type": "slide"} -->
Aller jusqu'√† ¬´‚ÄØ*Philosophy*‚ÄØ¬ª, version HTML
==================================================================

**Lo√Øc Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

2021-10-06
<!-- #endregion -->

```python
from IPython.display import display
```

## √ânonc√©

> *Wikipedia trivia: if you take any article, click on the first link in the article text not in
> parentheses or italics, **and then repeat**, you will eventually end up at "Philosophy".*
> (alt-text de [xkcd #903](https://xkcd.com/903/))

- V√©rifiez sur une page ou deux si c'est vrai
- √âcrivez un script qui prend en argument de ligne de commande un nom de page Wikip√©dia (en anglais,
  sauf si vous aimez l'aventure) et donne le nombre de sauts n√©cessaire pour arriver √† la page
  *Philosophy* ou une erreur si la page en question n'existe pas
- Si vous √™tes tr√®s d√©termin√©‚ãÖe‚ãÖs, faites un script qui prend en entr√©e des pages de Wikip√©dia et
  produit le graphe (orient√©) des pages obtenues en suivant √† chaque fois le premier lien de chaque
  page, et ce jusqu'√† retomber sur une page d√©j√† visit√©e. On pourra par exemple utiliser
  [NetworkX](https://networkx.org/documentation/latest/reference/drawing.html), un visualiseur
  interactif comme [pyvis](https://pyvis.readthedocs.io/en/latest/tutorial.html), [un wrapper de
  graphviz](https://graphviz.readthedocs.io) ou encore g√©n√©rer directement des fichiers dot.



On va d'abord commencer par jouer un peu avec les donn√©es, on fera un script apr√®s

## Introduction

On commence par s'assurer qu'on les bons outils

```python
%pip install -U beautifulsoup4 lxml matplotlib requests
```

```python
from bs4 import BeautifulSoup
import lxml
import requests
```

## Premier essai


On part de la page [Algorithmic Bias](https://en.wikipedia.org/wiki/Algorithmic_bias)

```python
url = "https://en.wikipedia.org/wiki/Algorithmic_bias"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'lxml')
soup
```

On s'int√©resse au corps de page

```python
soup.body
```

Et dans le corps de page au premier parragraphe qui contient du texte

```python
first_p = [p for p in soup.body.find_all("p") if p.text and not p.text.isspace()][0]
first_p
```

On r√©cup√®re le premier lien (pour l'instant sans plus de condition)

```python
first_link = first_p.a
display(first_link)
display(first_link["href"])
```

Attention son `href` est un lien relatif, on va devoir le r√©soudre

```python
from urllib.parse import urljoin
next_url = urljoin(url, first_link["href"])
next_url
```

On r√©cup√®re la page suivante

```python
response = requests.get(next_url)
soup = BeautifulSoup(response.text, 'lxml')
soup.title.text
```

(Il y a eu une pseudo-redirection mais pas via une requ√™te HTTP, plut√¥t via de la cuisine interne √†
MediaWiki et [de la sorcellerie](https://developer.mozilla.org/en-US/docs/Web/API/History/pushState)
en Javascript. Du coup, √ßa c'est pass√© sans douleur pour nous.)

## üîÑ

On recommence

```python
first_p = [p for p in soup.body.find_all("p") if p.text and not p.text.isspace()][0]
first_p
```

On a de la chance‚ÄØ: les bandeaux d'info et notices de redirection n'utilisent pas de balise `<p>`.
Ce n'est pas tr√®s robuste de se reposer l√†-dessus, mais comme de toute fa√ßon on reparse du HTML
g√©n√©r√© (et pas √©crit directement), et pas tr√®s propre, on ne sera que rarement robustes‚Ä¶


Plut√¥t que de refaire la m√™me sauce √† la mano, on va √©crire une fonction

```python
def get_next_url(url):
    """R√©cup√®re la page √† `url` et revoie l'addresse du premier lien du premier paragraphe"""
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    # Plut√¥t que de cr√©er une liste et de jeter tout sauf son premier √©l√©ment,
    # on fait √ßa avec un g√©n√©rateur
    first_p = next(p for p in soup.body.find_all("p") if p.text and not p.text.isspace())
    first_link = first_p.a
    return urljoin(url, first_link["href"])
```

On teste

```python
get_next_url(next_url)
```

Et encore

```python
for _ in range(16):
    print(next_url)
    next_url = get_next_url(next_url)
```

On voit que [Latin language](https://en.wikipedia.org/wiki/Latin) nous envoie sur
[Help:IPA/Latin](https://en.wikipedia.org/wiki/Help:IPA/Latin). √áa n'a pas l'air d'√™tre correct.

## Ne pars pas en th√®se


Regardons un peu ce que la page a dans le ventre

```python
response = requests.get("https://en.wikipedia.org/wiki/Latin")
soup = BeautifulSoup(response.text, 'lxml')
first_p = next(p for p in soup.body.find_all("p") if p.text and not p.text.isspace())
first_p
```

Le probl√®me a l'air d'√™tre que la condition ¬´‚ÄØlien pas entre parenth√®ses‚ÄØ¬ª n'est pas v√©rifi√©e ici.
Pour emp√™cher √ßa on va utiliser le crit√®re suivant‚ÄØ: un lien est entre parenth√®ses s'il est pr√©c√©d√©
par plus de parenth√®ses ouvrantes que de parenth√®ses fermantes. Pour savoir si un lien est entre
parenth√®ses, on va donc regarder tout le texte avant lui dans le paragraphe et compter les
parenth√®ses.

```python
# Le type des textes dans bs4
from bs4.element import NavigableString
# On a besoin de pr√©ciser `p` parce que ce n'est peut-√™tre pas le parent direct de `a`
def is_between_parentheses(a, p):
    n_open = 0
    for elem in p.descendants:
        if elem is a:
            break
        # On ne compte les parenth√®ses que dans les textes
        if isinstance(elem, NavigableString):
            n_open += elem.count("(") - elem.count(")")
    # S'il y a plus d'ouvrants que de fermants c'est pas notre probl√®me
    return n_open > 0
```

```python
first_a = next(a for a in first_p.find_all("a") if not is_between_parentheses(a, first_p))
first_a
```

√áa marche‚ÄØ!

##¬†I vecchi tag

Un dernier point ce sont les italiques. En examinant le code on trouve l'horreur suivante (qui se
trouve √™tre entre parenth√®ses donc ici √ßa ne g√™ne pas mais √ßa pourrait l'√™tre dans une autre page.


<!-- #region -->
```html
<p><b>Latin</b> (<i title="Latin-language text" lang="la">latƒ´num</i>
```
<!-- #endregion -->

Visiblement chez MediaWiki on a pas trop de complexes avec la non-s√©paration du style et du contenu
et pour mettre en italiques on utilise `<i>` (√ßa ne concerne pas les bandeau d'info qui eux sont
bien styl√©s en CSS). √áa tombe bien, on va pouvoir exploiter √ßa pour √™tre s√ªr‚ãÖe‚ãÖs de ne pas prendre
un lien en italiques‚ÄØ: on va chercher seulement les `<a>` qui ne sont pas dans un `<i>`

```python
first_a = next(
    a
    for a in first_p.find_all("a")
    if not a.find_parents("i") and not is_between_parentheses(a, first_p)
)
first_a
```

On remet tout √ßa dans notre fonction

```python
def get_next_url(url):
    """R√©cup√®re la page √† `url` et revoie l'addresse du premier lien du premier paragraphe"""
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    first_p = next(p for p in soup.body.find_all("p") if p.text and not p.text.isspace())
    first_link = next(
        a
        for a in first_p.find_all("a")
        if not a.find_parents("i") and not is_between_parentheses(a, first_p)
    )
    return urljoin(url, first_link["href"])
```

```python
next_url = "https://en.wikipedia.org/wiki/Algorithmic_bias"
for _ in range(16):
    print(next_url)
    next_url = get_next_url(next_url)
```

**argh**

## Fraught loops


Il y a une r√®gle √† laquelle Randall Munroe n'a pas pens√©‚ÄØ: les liens de r√©f√©rence, qui sont des
liens internes √† la page (ils pointent vers une URL qui ne contient qu'une
[ancre](https://developer.mozilla.org/en-US/docs/Learn/Common_questions/What_is_a_URL#anchor)) et
donc nous font tourner en rond. On va r√©gler √ßa sauvagement en prenant le premier lien qui ne
commence pas par `#`.

```python
def get_next_url(url):
    """R√©cup√®re la page √† `url` et revoie l'addresse du premier lien du premier paragraphe"""
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    first_p = next(p for p in soup.body.find_all("p") if p.text and not p.text.isspace())
    first_link = next(
        a
        for a in first_p.find_all("a")
        if not a["href"].startswith("#") and not a.find_parents("i") and not is_between_parentheses(a, first_p)
    )
    return urljoin(url, first_link["href"])
```

Note que √ßa ne nous prot√®ge pas des liens qui serait une redirection vers la m√™me page, mais avec un
nom diff√©rent. Mais on se pr√©occupera de √ßa si on tombe dessus, comme c'est probablement assez rare

```python tags=["raises-exception"]
next_url = "https://en.wikipedia.org/wiki/Algorithmic_bias"
for _ in range(16):
    print(next_url)
    next_url = get_next_url(next_url)
```

Encore‚ÄØ?

## Le dernier probl√®me

```python tags=["raises-exception"]
get_next_url("https://en.wikipedia.org/wiki/Logic")
```

Quel est le probl√®me cette fois-ci‚ÄØ? Visiblement le premier paragraphe ne contient pas de lien üò±


C'est effectivement un truc auquel on avait pas pens√© et qu'il faudra g√©rer, mais si on regarde bien
ce cas pr√©cis

```python
response = requests.get("https://en.wikipedia.org/wiki/Logic")
soup = BeautifulSoup(response.text, 'lxml')
first_p = next(p for p in soup.body.find_all("p") if p.text and not p.text.isspace())
first_p
```

C'est en fait un bout de la barre lat√©rale, pas le *vrai* premier paragraphe


On va r√©gler √ßa en cherchant nos liens dans les n≈ìuds enfants *directs* (comme √ßa on esquive tous
les widgets) du cadre de contenu qui est visiblement toujours de la forme `<div
class="mw-parser-output" ‚Ä¶>` et qu'on peut donc r√©cup√©rer avec un s√©lecteur CSS.

```python
def get_next_url(url):
    """R√©cup√®re la page √† `url` et revoie l'addresse du premier lien du premier paragraphe"""
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "lxml")
    
    # Un s√©lecteur CSS pour r√©cup√©rer le contenu de la page
    page_content = soup.select(".mw-parser-output")[0]
    first_p = next(
        p
        for p in page_content.find_all("p", recursive=False)
        if p.text and not p.text.isspace()
    )
    first_link = next(
        a
        for a in first_p.find_all("a")
        if not a["href"].startswith("#")
        and not a.find_parents("i")
        and not is_between_parentheses(a, first_p)
    )
    return urljoin(url, first_link["href"])
get_next_url("https://en.wikipedia.org/wiki/Logic")
```

```python
next_url = "https://en.wikipedia.org/wiki/Algorithmic_bias"
for _ in range(29):
    print(next_url)
    next_url = get_next_url(next_url)
```

ü•≥


On va quand m√™me pr√©voir l'avenir‚ÄØ: il se pourrait qu'une page n'ait pas de lien dans le premier
paragraphe. On va donc plut√¥t it√©rer sur les paragraphes

```python
def get_next_url(url):
    """R√©cup√®re la page √† `url` et revoie l'addresse du premier lien du premier paragraphe"""
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "lxml")
    page_content = soup.select(".mw-parser-output")[0]
    paragraphs = (
        p
        for p in page_content.find_all("p", recursive=False)
        if p.text and not p.text.isspace()
    )
    # On pourrait aussi faire une tr√®s grosse compr√©hension mais c'est d√©j√† assez compliqu√©
    for p in paragraphs:
        #¬†`None` en valeur par d√©faut si le g√©n√©rateur est vide
        first_link = next(
            (
                a
                for a in p.find_all("a")
                if not a["href"].startswith("#")
                and not a.find_parents("i")
                and not is_between_parentheses(a, p)
            ),
            None,
        )
        if first_link is not None:
            return urljoin(url, first_link["href"])

# Une page sans lien valide dans le premier paragraphe
get_next_url("https://en.wikipedia.org/wiki/Creativity")
```

## Le po√®te des cercles disparus


Il ne reste plus qu'√† √©crire une fonction qui

- Prend la page de d√©part en entr√©e
- Suis les liens jusqu'√†
  - Tomber sur *Philosophy*
  - Ou sur une page d√©j√† visit√©e (pour √©viter de tourner en rond)
- Renvoie le nombre de pages visit√©es

On *pourrait* le faire en r√©cursif, mais √ßa rend la d√©tection des cycles et le compte des sauts
p√©nibles alors on va simplement faire une boucle.

```python
import sys

# On va modifier la fonction `get_next_url` parce qu'on va se servir de la soupe pour d'autre choses
# et on veut √©viter de parser plusieurs fois la m√™me page
def get_first_url(soup):
    """Renvoie l'addresse du premier lien du premier paragraphe dans une soupe"""
    page_content = soup.select(".mw-parser-output")[0]
    paragraphs = (
        p
        for p in page_content.find_all("p", recursive=False)
        if p.text and not p.text.isspace()
    )
    for p in paragraphs:
        first_link = next(
            (
                a
                for a in p.find_all("a")
                if not a["href"].startswith("#")
                and not a.find_parents("i")
                and not is_between_parentheses(a, p)
            ),
            None,
        )
        if first_link is not None:
            return first_link["href"]


def search_for(start_url, target_title="Philosophy"):
    next_url = start_url
    # Les initialisations bizarres c'est parce qu'on va devoir faire au moins un tour de boucle pour
    # v√©rifier le titre de la page de d√©part (utile par exemple si on est dans une redirection)
    n_hops = -1
    title = None
    # √áa devrait √™tre marginalement plus rapide avec un set pour qui insertion et test d'inclusion
    # sont en moyenne O(1) qu'avec une liste qui est aussi en O(1) (mais plus rapide en moyenne)
    # pour l'insertion mais O(n) pour le test d'inclusion. Voir
    # <https://wiki.python.org/moin/TimeComplexity>
    visited = set()
    while title != target_title:
        n_hops += 1
        response = requests.get(next_url)
        soup = BeautifulSoup(response.text, "lxml")
        # Les titres sont de la forme  "{titre de page} - Wikipedia"
        title = soup.title.string.split(" - ")[0]
        # On va afficher le trajet, √ßa nous distraira. Pour des applications s√©rieuses, on utilisera
        # une bibli de log comm [loguru](https://loguru.readthedocs.io)
        print(title, file=sys.stderr)
        # On a trouv√© un cycle
        if title in visited:
            # On sait que c'est pas `target_title` sinon on serait sorti de la boucle avant,
            # puisqu'on l'a d√©j√† visit√©. On peut donc √™tre certain‚ãÖe‚ãÖs qu'on atteindra jamais la
            # cible. Plut√¥t que de faire des fantaisies √† renvoyer `math.inf` ou je ne sais quoi, on
            # va sobrement renvoyer `-1`. On pourrait aussi lever une exception.
            return -1
        next_url = urljoin(next_url, get_first_url(soup))
    return n_hops
```

```python
search_for("https://en.wikipedia.org/wiki/Algorithmic_bias")
```

Et on en fait [un script](wikipedia_soup.py)

```python
%run wikipedia_soup "https://en.wikipedia.org/wiki/Python_(programming_language)"
```

Regarde, on peut m√™me partir d'une page au hasard gr√¢ce √† <https://en.wikipedia.org/wiki/Special:Random>

```python
search_for("https://en.wikipedia.org/wiki/Special:Random")
```

## Des graphes des graphes des graphes

Si on est tr√®s motiv√©‚ãÖe‚ãÖs, on va essayer de visualiser la structure¬†que √ßa donne √† Wikip√©dia. Pour
√ßa on peut se servir de [ipycytoscape](https://github.com/cytoscape/ipycytoscape)

```python
%pip install ipycytoscape
```

```python
import ipycytoscape
import ipywidgets as widgets
import networkx as nx
```

Il s'agit d'un paquet pour visualiser interactivement des graphes dans des notebooks. Par exemple
avec [NetworkX](networkx.org) (qu'on peut utiliser en standalone pour des scripts)

```python
G = nx.complete_graph(5)
directed = ipycytoscape.CytoscapeWidget()
directed.graph.add_graph_from_networkx(G, directed=True)
directed
```

## Des refs‚ÄØ?

- [La page de Wikipedia sur ce sujet](https://en.wikipedia.org/wiki/Wikipedia:Getting_to_Philosophy)
- Un article qui traite d'une g√©n√©ralisation du probl√®me‚ÄØ:

  - Lamprecht, Daniel, Dimitar Dimitrov, Denis Helic, and Markus Strohmaier. [‚ÄòEvaluating and
    Improving Navigability of Wikipedia: A Comparative Study of Eight Language
    Editions‚Äô](https://dl.acm.org/doi/10.1145/2957792.2957813?cid=81100338950). In Proceedings of
    the 12th International Symposium on Open Collaboration, 1‚Äì10. OpenSym ‚Äô16. New York, NY, USA:
    Association for Computing Machinery, 2016. <https://doi.org/10.1145/2957792.2957813>.

